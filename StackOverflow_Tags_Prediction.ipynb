{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StackOverflow Tags Prediction",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niteshpd/ml_projects/blob/master/StackOverflow_Tags_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_v2jJAc5mQn",
        "colab_type": "text"
      },
      "source": [
        "# StackOverflow Tag Prediction\n",
        "\n",
        "### `Task:`\n",
        "Predict the tags (a.k.a. keywords, topics, summaries), given only the question text and its title. The dataset contains content from disparate stack exchange sites, containing a mix of both technical and non-technical questions.\n",
        "\n",
        "### `Evaluation Metric:`\n",
        "The evaluation metric for this project is Mean F1-Score.  The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision P and recall R. Precision is the ratio of true positives (TP) to all predicted positives (TP + FP). Recall is the ratio of true positives to all actual positives (TP + FN). The F1 score is given by:\n",
        "\n",
        "> F1=2PR / (P + R)\n",
        "\n",
        "The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.\n",
        "\n",
        "The tag predicted must be an exact match, regardless of whether the tags are synonyms. \n",
        "\n",
        "* **Why aren't synonyms counted?**\n",
        "\n",
        "1. Giving out a list of candidate synonyms is a potential source of leakage\n",
        "2. Synonyms are subjective, and there are \"subjectively many\" synonyms for a given tag\n",
        "3. Equally penalized for predicting a synonym of a correct tag, so the task can be framed as not only predicting a tag, but also modeling the distribution(s) of potential synonyms\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzhGZhkQ7YWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}